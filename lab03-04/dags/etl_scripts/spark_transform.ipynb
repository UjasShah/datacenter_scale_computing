{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from google.cloud import storage\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_map = {'Rto-Adopt':1, \n",
    "                'Adoption':2, \n",
    "                'Euthanasia':3, \n",
    "                'Transfer':4,\n",
    "                'Return to Owner':5, \n",
    "                'Died':6, \n",
    "                'Disposal':7,\n",
    "                'Missing':8,\n",
    "                'Relocate':9,\n",
    "                'N/A':10,\n",
    "                'Stolen':11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/17 17:47:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"animal_shelter\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+---------+-------------+---------------+---------------+-----------+----------------+----------------+--------------------+------------------+\n",
      "|Animal ID|      Name|            DateTime|MonthYear|Date of Birth|   Outcome Type|Outcome Subtype|Animal Type|Sex upon Outcome|Age upon Outcome|               Breed|             Color|\n",
      "+---------+----------+--------------------+---------+-------------+---------------+---------------+-----------+----------------+----------------+--------------------+------------------+\n",
      "|  A794011|     Chunk|05/08/2019 06:20:...| May 2019|   05/02/2017|      Rto-Adopt|           NULL|        Cat|   Neutered Male|         2 years|Domestic Shorthai...| Brown Tabby/White|\n",
      "|  A776359|     Gizmo|07/18/2018 04:02:...| Jul 2018|   07/12/2017|       Adoption|           NULL|        Dog|   Neutered Male|          1 year|Chihuahua Shortha...|       White/Brown|\n",
      "|  A821648|      NULL|08/16/2020 11:38:...| Aug 2020|   08/16/2019|     Euthanasia|           NULL|      Other|         Unknown|          1 year|             Raccoon|              Gray|\n",
      "|  A720371|     Moose|02/13/2016 05:59:...| Feb 2016|   10/08/2015|       Adoption|           NULL|        Dog|   Neutered Male|        4 months|Anatol Shepherd/L...|              Buff|\n",
      "|  A674754|      NULL|03/18/2014 11:47:...| Mar 2014|   03/12/2014|       Transfer|        Partner|        Cat|     Intact Male|          6 days|Domestic Shorthai...|      Orange Tabby|\n",
      "|  A659412|  Princess|10/05/2020 02:37:...| Oct 2020|   03/24/2013|       Adoption|           NULL|        Dog|   Spayed Female|         7 years|Chihuahua Shortha...|             Brown|\n",
      "|  A814515|   Quentin|05/06/2020 07:59:...| May 2020|   03/01/2018|       Adoption|         Foster|        Dog|   Neutered Male|         2 years|American Foxhound...|       White/Brown|\n",
      "|  A881795|      NULL|06/13/2023 07:13:...| Jun 2023|   05/26/2021|       Transfer|            Snr|        Cat|         Unknown|         2 years|  Domestic Shorthair|       Brown Tabby|\n",
      "|  A689724|*Donatello|10/18/2014 06:52:...| Oct 2014|   08/01/2014|       Adoption|           NULL|        Cat|   Neutered Male|        2 months|Domestic Shorthai...|             Black|\n",
      "|  A680969|     *Zeus|08/05/2014 04:59:...| Aug 2014|   06/03/2014|       Adoption|           NULL|        Cat|   Neutered Male|        2 months|Domestic Shorthai...|White/Orange Tabby|\n",
      "|  A840370|     Tulip|08/19/2021 07:36:...| Aug 2021|   08/06/2019|       Adoption|           NULL|        Dog|   Spayed Female|         2 years|Border Collie/Car...|       Black/White|\n",
      "|  A684617|      NULL|07/27/2014 09:00:...| Jul 2014|   07/26/2012|       Transfer|           SCRP|        Cat|   Intact Female|         2 years|Domestic Shorthai...|             Black|\n",
      "|  A742354|   Artemis|01/22/2017 11:56:...| Jan 2017|   01/20/2010|Return to Owner|           NULL|        Cat|   Neutered Male|         7 years|Domestic Shorthai...|        Blue/White|\n",
      "|  A818049|     Fiona|06/01/2020 01:24:...| Jun 2020|   06/01/2018|Return to Owner|           NULL|        Dog|   Intact Female|         2 years|            Pit Bull|        White/Blue|\n",
      "|  A843327|     *Mary|10/08/2021 01:25:...| Oct 2021|   09/29/2019|       Transfer|      Out State|        Dog|   Intact Female|         2 years|Chihuahua Shortha...|       Black/White|\n",
      "|  A681036|      NULL|06/11/2014 05:11:...| Jun 2014|   06/09/2014|       Transfer|        Partner|        Cat|     Intact Male|          2 days|Domestic Shorthai...|       Brown Tabby|\n",
      "|  A803149|    *Birch|08/31/2019 04:26:...| Aug 2019|   08/08/2019|       Transfer|        Partner|        Cat|     Intact Male|         3 weeks|  Domestic Shorthair|       Brown Tabby|\n",
      "|  A882456|      NULL|06/13/2023 07:14:...| Jun 2023|   06/05/2021|       Transfer|            Snr|        Cat|         Unknown|         2 years|  Domestic Shorthair|        Blue Tabby|\n",
      "|  A698049|     Luigi|03/16/2015 02:50:...| Mar 2015|   06/05/2014|       Transfer|        Partner|        Cat|   Spayed Female|        9 months|Domestic Medium H...|       Black/White|\n",
      "|  A773792|      NULL|06/05/2018 03:30:...| Jun 2018|   05/05/2018|     Euthanasia|      Suffering|        Cat|     Intact Male|         4 weeks|Domestic Shorthai...|      Orange Tabby|\n",
      "+---------+----------+--------------------+---------+-------------+---------------+---------------+-----------+----------------+----------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This needs to change\n",
    "df = spark.read.csv(f\"/Users/ujas/Downloads/Austin_Animal_Center_Outcomes.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCS stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(date):\n",
    "    # some more gcs stuff\n",
    "\n",
    "    data = spark.read.csv(f\"/Users/ujas/Downloads/Austin_Animal_Center_Outcomes.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    data = prep_data(data)\n",
    "\n",
    "    dim_animal_agg = prep_animal_dim(data)\n",
    "    dim_dates = prep_date_dim(data)\n",
    "    dim_outcome_types = prep_outcome_types_dim(data)\n",
    "    fct_outcomes = prep_outcomes_fct(data)\n",
    "\n",
    "    # upload to GCS\n",
    "\n",
    "def prep_data(data):\n",
    "    # removing stars from animal names\n",
    "    data = data.withColumn(\"Name\", F.regexp_replace(F.col(\"Name\"), \"\\*\", \"\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "SparkRuntimeException",
     "evalue": "[UNSUPPORTED_FEATURE.LITERAL_TYPE] The feature is not supported: Literal for '[Unknown, Intact Male, Intact Female, Spayed Female, Neutered Male]' of class java.util.HashSet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSparkRuntimeException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 93\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outcomes_fct\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[43mtransform_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-11-21\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m, in \u001b[0;36mtransform_data\u001b[0;34m(date)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Read data directly from GCS into a DataFrame\u001b[39;00m\n\u001b[1;32m     22\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/ujas/Downloads/Austin_Animal_Center_Outcomes.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mprep_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m dim_animal_agg \u001b[38;5;241m=\u001b[39m prep_animal_dim(df)\n\u001b[1;32m     27\u001b[0m dim_dates \u001b[38;5;241m=\u001b[39m prep_date_dim(df)\n",
      "Cell \u001b[0;32mIn[7], line 47\u001b[0m, in \u001b[0;36mprep_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     44\u001b[0m sex_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeutered Male\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntact Male\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntact Female\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpayed Female\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m     45\u001b[0m is_fixed_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeutered Male\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntact Male\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntact Female\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpayed Female\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[0;32m---> 47\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m\"\u001b[39m, when(\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msex_upon_outcome\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msex_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex_upon_outcome\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m     48\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_fixed\u001b[39m\u001b[38;5;124m\"\u001b[39m, when(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex_upon_outcome\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39misin(is_fixed_map\u001b[38;5;241m.\u001b[39mkeys()), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msex_upon_outcome\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Converting dates and times\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datacenter/lib/python3.10/site-packages/pyspark/sql/column.py:972\u001b[0m, in \u001b[0;36mColumn.isin\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mset\u001b[39m)):\n\u001b[1;32m    969\u001b[0m     cols \u001b[38;5;241m=\u001b[39m cast(Tuple, cols[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    970\u001b[0m cols \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    971\u001b[0m     Tuple,\n\u001b[0;32m--> 972\u001b[0m     [c\u001b[38;5;241m.\u001b[39m_jc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01melse\u001b[39;00m _create_column_from_literal(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols],\n\u001b[1;32m    973\u001b[0m )\n\u001b[1;32m    974\u001b[0m sc \u001b[38;5;241m=\u001b[39m get_active_spark_context()\n\u001b[1;32m    975\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misin\u001b[39m\u001b[38;5;124m\"\u001b[39m)(_to_seq(sc, cols))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datacenter/lib/python3.10/site-packages/pyspark/sql/column.py:972\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mset\u001b[39m)):\n\u001b[1;32m    969\u001b[0m     cols \u001b[38;5;241m=\u001b[39m cast(Tuple, cols[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    970\u001b[0m cols \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    971\u001b[0m     Tuple,\n\u001b[0;32m--> 972\u001b[0m     [c\u001b[38;5;241m.\u001b[39m_jc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_create_column_from_literal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols],\n\u001b[1;32m    973\u001b[0m )\n\u001b[1;32m    974\u001b[0m sc \u001b[38;5;241m=\u001b[39m get_active_spark_context()\n\u001b[1;32m    975\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misin\u001b[39m\u001b[38;5;124m\"\u001b[39m)(_to_seq(sc, cols))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datacenter/lib/python3.10/site-packages/pyspark/sql/column.py:51\u001b[0m, in \u001b[0;36m_create_column_from_literal\u001b[0;34m(literal)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_column_from_literal\u001b[39m(literal: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLiteralType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecimalLiteral\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     50\u001b[0m     sc \u001b[38;5;241m=\u001b[39m get_active_spark_context()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJVMView\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mliteral\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datacenter/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datacenter/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mSparkRuntimeException\u001b[0m: [UNSUPPORTED_FEATURE.LITERAL_TYPE] The feature is not supported: Literal for '[Unknown, Intact Male, Intact Female, Spayed Female, Neutered Male]' of class java.util.HashSet."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, date_format, concat_ws, when\n",
    "from google.cloud import storage\n",
    "from pyspark.sql.functions import year, month, dayofmonth, regexp_replace\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() #load environment variables\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"DataTransformation\").getOrCreate()\n",
    "\n",
    "# GCS stuff\n",
    "storage_client = storage.Client('oceanic-hangout-406022')\n",
    "bucket = storage_client.bucket('outcomes_bucket')\n",
    "\n",
    "def transform_data(date):\n",
    "    # Check if the blob exists\n",
    "    blob = bucket.blob(f'extracted/{date}_outcomes.csv')\n",
    "    if not blob.exists():\n",
    "        return\n",
    "\n",
    "    # Read data directly from GCS into a DataFrame\n",
    "    df = spark.read.csv(f\"/Users/ujas/Downloads/Austin_Animal_Center_Outcomes.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    df = prep_data(df)\n",
    "\n",
    "    dim_animal_agg = prep_animal_dim(df)\n",
    "    dim_dates = prep_date_dim(df)\n",
    "    dim_outcome_types = prep_outcome_types_dim(df)\n",
    "    fct_outcomes = prep_outcomes_fct(df)\n",
    "\n",
    "    # Upload transformed data to GCS\n",
    "    # Note: You may need to adjust this for your environment and ensure the necessary permissions\n",
    "    # This is a basic example of writing to GCS\n",
    "    # dim_animal_agg.write.csv(f\"gs://outcomes_bucket/transformed/dim_animal_agg.csv\", header=True)\n",
    "    # dim_dates.write.csv(f\"gs://outcomes_bucket/transformed/{date}_dim_dates.csv\", header=True)\n",
    "    # dim_outcome_types.write.csv(f\"gs://outcomes_bucket/transformed/{date}_dim_outcome_types.csv\", header=True)\n",
    "    # fct_outcomes.write.csv(f\"gs://outcomes_bucket/transformed/{date}_fct_outcomes.csv\", header=True)\n",
    "\n",
    "def prep_data(df):\n",
    "    # Apply transformations similar to the Pandas version\n",
    "    df = df.withColumn(\"name\", regexp_replace(col(\"name\"), \"\\*\", \"\"))\n",
    "    \n",
    "    # Mapping for 'sex' and 'is_fixed'\n",
    "    sex_map = {\"Neutered Male\":\"M\", \"Intact Male\":\"M\", \"Intact Female\":\"F\", \"Spayed Female\":\"F\", \"Unknown\":None}\n",
    "    is_fixed_map = {\"Neutered Male\":True, \"Intact Male\":False, \"Intact Female\":False, \"Spayed Female\":True, \"Unknown\":None}\n",
    "\n",
    "    df = df.withColumn(\"sex\", when(col(\"sex_upon_outcome\").isin(sex_map.keys()), col(\"sex_upon_outcome\")).otherwise(None))\n",
    "    df = df.withColumn(\"is_fixed\", when(col(\"sex_upon_outcome\").isin(is_fixed_map.keys()), col(\"sex_upon_outcome\")).otherwise(None))\n",
    "\n",
    "    # Converting dates and times\n",
    "    df = df.withColumn(\"ts\", to_date(col(\"datetime\")))\n",
    "    df = df.withColumn(\"date_id\", date_format(col(\"ts\"), 'yyyyMMdd'))\n",
    "    df = df.withColumn(\"time\", date_format(col(\"ts\"), 'HH:mm:ss'))\n",
    "\n",
    "    # Replace outcome_type with outcome_type_id\n",
    "    for outcome, id in outcomes_map.items():\n",
    "        df = df.withColumn('outcome_type_id', when(col('outcome_type') == outcome, id).otherwise(col('outcome_type_id')))\n",
    "\n",
    "    df = df.drop('outcome_type').withColumnRenamed('outcome_type_id', 'outcome_type')\n",
    "\n",
    "    return df\n",
    "\n",
    "def prep_animal_dim(df):\n",
    "    animal_dim = df.select(\"animal_id\", \"name\", \"date_of_birth\", \"sex\", \"animal_type\", \"breed\", \"color\")\n",
    "    animal_dim = animal_dim.withColumnRenamed(\"date_of_birth\", \"dob\")\n",
    "\n",
    "    # Load existing data if exists, and append\n",
    "    # Implement logic similar to the Pandas version using PySpark functions\n",
    "    # Example (may need adjustments based on your specific requirements):\n",
    "    # dim_animal_agg = spark.read.csv(\"existing_path\").union(animal_dim).dropDuplicates()\n",
    "\n",
    "    return animal_dim.dropDuplicates()\n",
    "\n",
    "def prep_date_dim(df):\n",
    "    dates_dim = df.select(\n",
    "        date_format(col(\"ts\"), 'yyyyMMdd').alias(\"date_id\"),\n",
    "        date_format(col(\"ts\"), 'yyyy-MM-dd').alias(\"date\"),\n",
    "        year(col(\"ts\")).alias(\"year\"),\n",
    "        month(col(\"ts\")).alias(\"month\"),\n",
    "        dayofmonth(col(\"ts\")).alias(\"day\")\n",
    "    )\n",
    "    return dates_dim.dropDuplicates()\n",
    "\n",
    "def prep_outcome_types_dim(df):\n",
    "    outcome_types_dim = spark.createDataFrame(outcomes_map.items(), [\"outcome_type\", \"outcome_type_id\"])\n",
    "    return outcome_types_dim\n",
    "\n",
    "def prep_outcomes_fct(df):\n",
    "    outcomes_fct = df.select(\"animal_id\", \"date_id\", \"time\", \"outcome_type_id\", \"outcome_subtype\", \"is_fixed\")\n",
    "    return outcomes_fct\n",
    "\n",
    "# Example usage\n",
    "transform_data('2023-11-21')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------------+---------+-------------+------------+---------------+-----------+----------------+----------------+--------------------+-----------------+\n",
      "|Animal ID| Name|            DateTime|MonthYear|Date of Birth|Outcome Type|Outcome Subtype|Animal Type|Sex upon Outcome|Age upon Outcome|               Breed|            Color|\n",
      "+---------+-----+--------------------+---------+-------------+------------+---------------+-----------+----------------+----------------+--------------------+-----------------+\n",
      "|  A794011|Chunk|05/08/2019 06:20:...| May 2019|   05/02/2017|   Rto-Adopt|           NULL|        Cat|   Neutered Male|         2 years|Domestic Shorthai...|Brown Tabby/White|\n",
      "|  A776359|Gizmo|07/18/2018 04:02:...| Jul 2018|   07/12/2017|    Adoption|           NULL|        Dog|   Neutered Male|          1 year|Chihuahua Shortha...|      White/Brown|\n",
      "|  A821648| NULL|08/16/2020 11:38:...| Aug 2020|   08/16/2019|  Euthanasia|           NULL|      Other|         Unknown|          1 year|             Raccoon|             Gray|\n",
      "|  A720371|Moose|02/13/2016 05:59:...| Feb 2016|   10/08/2015|    Adoption|           NULL|        Dog|   Neutered Male|        4 months|Anatol Shepherd/L...|             Buff|\n",
      "|  A674754| NULL|03/18/2014 11:47:...| Mar 2014|   03/12/2014|    Transfer|        Partner|        Cat|     Intact Male|          6 days|Domestic Shorthai...|     Orange Tabby|\n",
      "+---------+-----+--------------------+---------+-------------+------------+---------------+-----------+----------------+----------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize spark\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"shelter\").getOrCreate()\n",
    "\n",
    "# read in data\n",
    "data = spark.read.csv(\"/Users/ujas/Downloads/Austin_Animal_Center_Outcomes.csv\", header=True, inferSchema=True)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, to_timestamp, date_format\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"transform\").getOrCreate()\n",
    "\n",
    "def read_data_from_gcs(date):\n",
    "    file_path = f\"gs://outcomes_bucket/extracted/{date}_outcomes.csv\"\n",
    "    return spark.read.csv(file_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data):\n",
    "    data = data.withColumn(\"name\", col(\"name\").replace(\"*\", \"\", \"regex\"))\n",
    "    data = data.withColumn(\"sex\", col(\"sex_upon_outcome\").replace({\"Neutered Male\": \"M\", \"Intact Male\": \"M\", \"Intact Female\": \"F\", \"Spayed Female\": \"F\", \"Unknown\": None}))\n",
    "    data = data.withColumn(\"is_fixed\", col(\"sex_upon_outcome\").replace({\"Neutered Male\": True, \"Intact Male\": False, \"Intact Female\": False, \"Spayed Female\": True, \"Unknown\": None}))\n",
    "    data = data.withColumn(\"ts\", to_timestamp(col(\"datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    data = data.withColumn(\"date_id\", date_format(col(\"ts\"), \"yyyyMMdd\"))\n",
    "    data = data.withColumn(\"time\", date_format(col(\"ts\"), \"HH:mm:ss\"))\n",
    "    data = data.withColumn(\"outcome_type_id\", col(\"outcome_type\").replace(outcomes_map))\n",
    "    return data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacenter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
